{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import keras\n",
    "from keras import losses, metrics, layers, ops\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_df = train_df[['agent1', 'agent2', 'EnglishRules', 'LudRules']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1328/1328 [00:04<00:00, 272.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lud_preprocessor = layers.TextVectorization(\n",
    "    vocabulary='ludii_tokens.dic',\n",
    "    standardize='strip_punctuation'\n",
    ")\n",
    "\n",
    "eng_preprocessor = layers.TextVectorization(\n",
    "    vocabulary='eng_tokens.dic'\n",
    ")\n",
    "\n",
    "max_eng = 0\n",
    "max_lud = 0\n",
    "\n",
    "eng_uniques = train_set_df['EnglishRules'].unique()\n",
    "\n",
    "for engrul in tqdm(eng_uniques):\n",
    "    eng_vector = eng_preprocessor(engrul)\n",
    "    max_eng = max(max_eng, eng_vector.shape[0])\n",
    "\n",
    "print(max_eng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23613\n"
     ]
    }
   ],
   "source": [
    "lud_uniques = train_set_df['LudRules'].unique()\n",
    "\n",
    "EQPMNT_RE = r'\\(equipment'\n",
    "RULESTRIP_RE = r'[^a-zA-Z\\(\\)\\{\\}]'\n",
    "\n",
    "for rule in lud_uniques:\n",
    "    start = re.search(EQPMNT_RE, rule).span()[0]\n",
    "    pure_rule = rule[start:]\n",
    "    pure_rule = re.sub(RULESTRIP_RE, ' ', pure_rule)\n",
    "    lud_vector = lud_preprocessor(pure_rule)\n",
    "    max_lud = max(max_lud, lud_vector.shape[0])\n",
    "\n",
    "print(max_lud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent1_uniques = train_set_df['agent1'].unique()\n",
    "agent2_uniques = train_set_df['agent2'].unique()\n",
    "\n",
    "set(agent1_uniques) >= set(agent2_uniques)\n",
    "\n",
    "code_agent = {}\n",
    "agent_code = {}\n",
    "\n",
    "for id, agent in enumerate(agent1_uniques):\n",
    "    code_agent[id] = agent\n",
    "    agent_code[agent] = id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eng_vector.shape=(900,)\n",
    "\n",
    "lud_vector.shape=(23700,)\n",
    "\n",
    "agents_to_categorical.shape=(144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 900+23700+144 = 24744\n",
    "# defining input shape and input data distribution\n",
    "\n",
    "agents_len = 144\n",
    "engvector_len = 900\n",
    "ludvector_len = 23700\n",
    "\n",
    "enter = layers.Input(shape=(24744,))\n",
    "agent_input_layer = layers.Input(shape=(agents_len,))\n",
    "agent_input_data = ops.slice(enter, (0,), (agents_len,))\n",
    "\n",
    "engrul_input_data = ops.slice(enter, (agents_len,), (engvector_len,))\n",
    "\n",
    "ludrul_input_data = ops.slice(enter, (agents_len+engvector_len,), (ludvector_len,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English rules LSTM features extractor.\n",
    "# \n",
    "# Train set contains 384 preextracted features.\n",
    "# Let`s assume that game rule contains these features and ather that describes \n",
    "# Number of these features is unknown and needs to be discovered\n",
    "# presume that total amount of fetures, that are important for game result prediction is not less then double amount \n",
    "# of preextracted features and to be 800\n",
    "\n",
    "FEATURES_NUM = 800\n",
    "ENG_VECTOR_DIM = 512\n",
    "LUD_VECTOR_DIM = 768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EngRule LSTM\n",
    "\n",
    "engrul_input_layer = layers.Input(shape=(engvector_len,))\n",
    "eng_emb = layers.Embedding(input_dim=3692, output_dim=ENG_VECTOR_DIM)(engrul_input_layer)\n",
    "eng_x = layers.Bidirectional(layers.LSTM(ENG_VECTOR_DIM, return_sequences=True))(eng_emb)\n",
    "eng_x = layers.Bidirectional(layers.LSTM(ENG_VECTOR_DIM))(eng_x)\n",
    "eng_out = layers.Dense(FEATURES_NUM, activation='relu')(eng_x)\n",
    "\n",
    "# model = keras.Model(inputs=[enter], outputs=[eng_out])\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LudRule LSTM\n",
    "\n",
    "ludrul_input_layer = layers.Input(shape=(ludvector_len,))\n",
    "lud_emb = layers.Embedding(input_dim=1240, output_dim=LUD_VECTOR_DIM)(ludrul_input_layer)\n",
    "lud_x = layers.Bidirectional(layers.LSTM(LUD_VECTOR_DIM, return_sequences=True))(lud_emb)\n",
    "lud_x = layers.Bidirectional(layers.LSTM(LUD_VECTOR_DIM))(lud_x)\n",
    "lud_out = layers.Dense(FEATURES_NUM, activation='relu')(lud_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assemble = layers.Concatenate(axis=1)([agent_input_layer, eng_out, lud_out])\n",
    "x = layers.Dense(1200, activation='relu')(assemble)\n",
    "x = layers.Dense(512, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "out = layers.Dense(1, activation='tanh')(x)\n",
    "\n",
    "model = keras.Model(inputs=[agent_input_layer, engrul_input_layer, ludrul_input_layer],\n",
    "                    outputs=[out])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.utils.plot_model(model,\n",
    "#                        show_shapes=True,\n",
    "#                        expand_nested=True,\n",
    "#                        show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GameDataGenerator(keras.utils.Sequence):\n",
    "    eqpmnt_re = r'\\(equipment'\n",
    "    rulestrip_re = r'[^a-zA-Z\\(\\)\\{\\}]'\n",
    "    engvector_len = 900\n",
    "    ludvector_len = 23700\n",
    "    \n",
    "    def __init__(self, list_IDs, dataframe, labels_dict, batch_size=32, shuffle=True):\n",
    "        self.batch_size = batch_size\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels_dict = labels_dict\n",
    "        self.df = dataframe[['agent1', 'agent2', 'EnglishRules', 'LudRules']]\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        self._agent_code = agent_code\n",
    "    \n",
    "    @property\n",
    "    def agent_code(self):\n",
    "        return self._agent_code\n",
    "\n",
    "    @agent_code.setter\n",
    "    def agent_code(self):\n",
    "        agents = self.df['agent1'].unique().tolist()\n",
    "        agents.sort()\n",
    "        ret = {}\n",
    "        for id_, agent in enumerate(agents):\n",
    "            ret[agent] = id_\n",
    "        self._agent_code = ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, batch_num):\n",
    "          # Generate indexes of the batch\n",
    "        indexes = self.indexes[batch_num*self.batch_size:(batch_num+1)*self.batch_size]\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_ends(self):\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def _lud_prepare(self, rule) -> str:\n",
    "        start = re.search(self.eqpmnt_re, rule).span()[0]\n",
    "        pure_rule = rule[start:]\n",
    "        pure_rule = re.sub(self.rulestrip_re, ' ', pure_rule)\n",
    "        return pure_rule\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X_agents = []\n",
    "        X_engvectors = []\n",
    "        X_ludvectors = []\n",
    "        y = []\n",
    "\n",
    "        agents_num = len(self.agent_code)\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            row = self.df.loc[ID]\n",
    "            agent1, agent2, engrul, ludrul = row\n",
    "\n",
    "            y.append(self.labels_dict[ID])\n",
    "\n",
    "            agent1_encoded = keras.utils.to_categorical(self.agent_code[agent1], agents_num)\n",
    "            agent2_encoded = keras.utils.to_categorical(self.agent_code[agent2], agents_num)\n",
    "            agents_encoded = np.hstack((agent1_encoded, agent2_encoded))\n",
    "            X_agents.append(np.array(agents_encoded))\n",
    "\n",
    "            engrul_vector = eng_preprocessor(engrul)\n",
    "            engrul_vector = keras.utils.pad_sequences((engrul_vector,), maxlen=self.engvector_len)\n",
    "            X_engvectors.append(engrul_vector)\n",
    "\n",
    "            ludrul_vector = lud_preprocessor(self._lud_prepare(ludrul))\n",
    "            ludrul_vector = keras.utils.pad_sequences((ludrul_vector,), maxlen=self.ludvector_len)\n",
    "            X_ludvectors.append(ludrul_vector)\n",
    "\n",
    "        return (X_agents, X_engvectors, X_ludvectors), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shuffled_df = shuffle(train_df)\n",
    "\n",
    "labels = train_shuffled_df['utility_agent1'].to_dict()\n",
    "\n",
    "X_train, X_test = train_test_split(train_shuffled_df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([144185,  69399,    943, 138781, 174978, 127630, 223965, 224209,  21197,\n",
       "       101847,\n",
       "       ...\n",
       "       180259, 194726,  27271, 170807, 134479, 125608,  73541,  56872,   6208,\n",
       "        38458],\n",
       "      dtype='int64', length=174925)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = GameDataGenerator(X_train.index, train_shuffled_df, labels)\n",
    "test_generator = GameDataGenerator(X_test.index, train_shuffled_df, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=losses.MeanSquaredError,\n",
    "              metrics=[metrics.MeanSquaredError]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`output_signature` must contain objects that are subclass of `tf.TypeSpec` but found <class 'list'> which is not.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ds_atwork\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\ds_atwork\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_generator_op.py:124\u001b[0m, in \u001b[0;36m_from_generator\u001b[1;34m(generator, output_types, output_shapes, args, output_signature, name)\u001b[0m\n\u001b[0;32m    122\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m spec \u001b[38;5;129;01min\u001b[39;00m nest\u001b[38;5;241m.\u001b[39mflatten(output_signature):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec, type_spec\u001b[38;5;241m.\u001b[39mTypeSpec):\n\u001b[1;32m--> 124\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`output_signature` must contain objects that are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubclass of `tf.TypeSpec` but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(spec)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m output_types \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mTypeError\u001b[0m: `output_signature` must contain objects that are subclass of `tf.TypeSpec` but found <class 'list'> which is not."
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_atwork",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
